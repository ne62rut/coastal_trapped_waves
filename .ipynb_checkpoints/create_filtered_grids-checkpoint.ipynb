{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c103ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_149.10.ncSaved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_149.10.ncSaved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_149.00.nc\n",
      "\n",
      "\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-32.70_lon_149.10.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_149.10.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_149.00.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_149.20.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_149.00.nc\n",
      "Processed point at lat: -38.0, lon: 149.0\n",
      "Processed point at lat: -37.900001525878906, lon: 149.0\n",
      "Processed point at lat: -37.79999923706055, lon: 149.0\n",
      "Processed point at lat: -38.0, lon: 149.1\n",
      "Processed point at lat: -37.900001525878906, lon: 149.1\n",
      "Processed point at lat: -37.79999923706055, lon: 149.1\n",
      "Processed point at lat: -32.70000076293945, lon: 149.1\n",
      "Processed point at lat: -38.0, lon: 149.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_149.30.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_149.20.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_149.30.nc\n",
      "Processed point at lat: -37.900001525878906, lon: 149.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_149.40.ncSaved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_149.20.nc\n",
      "\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-32.80_lon_149.20.ncSaved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_149.30.nc\n",
      "\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_149.40.nc\n",
      "Processed point at lat: -37.79999923706055, lon: 149.20000000000002\n",
      "Processed point at lat: -32.79999923706055, lon: 149.20000000000002\n",
      "Processed point at lat: -38.0, lon: 149.3\n",
      "Processed point at lat: -37.900001525878906, lon: 149.3\n",
      "Processed point at lat: -37.79999923706055, lon: 149.3\n",
      "Processed point at lat: -38.0, lon: 149.4\n",
      "Processed point at lat: -37.900001525878906, lon: 149.4\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_149.40.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_149.50.nc\n",
      "Processed point at lat: -37.79999923706055, lon: 149.4\n",
      "Processed point at lat: -38.0, lon: 149.5\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_149.50.nc\n",
      "Processed point at lat: -37.900001525878906, lon: 149.5\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_149.60.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_149.60.ncSaved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_149.70.nc\n",
      "\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_149.50.ncSaved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_149.60.nc\n",
      "\n",
      "Processed point at lat: -37.79999923706055, lon: 149.5\n",
      "Processed point at lat: -38.0, lon: 149.6\n",
      "Processed point at lat: -37.900001525878906, lon: 149.6\n",
      "Processed point at lat: -37.79999923706055, lon: 149.6\n",
      "Processed point at lat: -38.0, lon: 149.70000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.70_lon_149.70.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_149.70.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_149.70.nc\n",
      "Processed point at lat: -37.900001525878906, lon: 149.70000000000002\n",
      "Processed point at lat: -37.79999923706055, lon: 149.70000000000002\n",
      "Processed point at lat: -37.70000076293945, lon: 149.70000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.60_lon_149.80.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_149.80.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_149.80.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_149.80.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.70_lon_149.80.nc\n",
      "Processed point at lat: -38.0, lon: 149.8\n",
      "Processed point at lat: -37.900001525878906, lon: 149.8\n",
      "Processed point at lat: -37.79999923706055, lon: 149.8\n",
      "Processed point at lat: -37.70000076293945, lon: 149.8\n",
      "Processed point at lat: -37.599998474121094, lon: 149.8\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.50_lon_149.80.nc\n",
      "Processed point at lat: -37.5, lon: 149.8\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_149.90.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_149.90.nc\n",
      "Processed point at lat: -38.0, lon: 149.9\n",
      "Processed point at lat: -37.900001525878906, lon: 149.9\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.70_lon_149.90.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_149.90.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.60_lon_149.90.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.10_lon_149.90.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.90_lon_149.90.nc\n",
      "Processed point at lat: -37.79999923706055, lon: 149.9\n",
      "Processed point at lat: -37.70000076293945, lon: 149.9\n",
      "Processed point at lat: -37.599998474121094, lon: 149.9\n",
      "Processed point at lat: -37.099998474121094, lon: 149.9\n",
      "Processed point at lat: -36.900001525878906, lon: 149.9\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_150.00.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_150.00.nc\n",
      "Processed point at lat: -38.0, lon: 150.0\n",
      "Processed point at lat: -37.900001525878906, lon: 150.0\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_150.00.nc\n",
      "Processed point at lat: -37.79999923706055, lon: 150.0\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.60_lon_150.00.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.70_lon_150.00.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.30_lon_150.00.nc\n",
      "Processed point at lat: -37.70000076293945, lon: 150.0\n",
      "Processed point at lat: -37.599998474121094, lon: 150.0\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.50_lon_150.00.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.40_lon_150.00.nc\n",
      "Processed point at lat: -37.5, lon: 150.0\n",
      "Processed point at lat: -37.400001525878906, lon: 150.0\n",
      "Processed point at lat: -37.29999923706055, lon: 150.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.10_lon_150.00.nc\n",
      "Processed point at lat: -37.099998474121094, lon: 150.0\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.00_lon_150.00.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.90_lon_150.00.nc\n",
      "Processed point at lat: -37.0, lon: 150.0\n",
      "Processed point at lat: -36.900001525878906, lon: 150.0\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.80_lon_150.00.nc\n",
      "Processed point at lat: -36.79999923706055, lon: 150.0\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.70_lon_150.00.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_150.10.nc\n",
      "Processed point at lat: -36.70000076293945, lon: 150.0\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_150.10.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_150.10.nc\n",
      "Processed point at lat: -38.0, lon: 150.1\n",
      "Processed point at lat: -37.900001525878906, lon: 150.1\n",
      "Processed point at lat: -37.79999923706055, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.70_lon_150.10.nc\n",
      "Processed point at lat: -37.70000076293945, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.60_lon_150.10.nc\n",
      "Processed point at lat: -37.599998474121094, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.50_lon_150.10.nc\n",
      "Processed point at lat: -37.5, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.40_lon_150.10.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.30_lon_150.10.nc\n",
      "Processed point at lat: -37.400001525878906, lon: 150.1\n",
      "Processed point at lat: -37.29999923706055, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.20_lon_150.10.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.10_lon_150.10.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.00_lon_150.10.nc\n",
      "Processed point at lat: -37.20000076293945, lon: 150.1\n",
      "Processed point at lat: -37.099998474121094, lon: 150.1\n",
      "Processed point at lat: -37.0, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.90_lon_150.10.nc\n",
      "Processed point at lat: -36.900001525878906, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.80_lon_150.10.nc\n",
      "Processed point at lat: -36.79999923706055, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.70_lon_150.10.nc\n",
      "Processed point at lat: -36.70000076293945, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.60_lon_150.10.nc\n",
      "Processed point at lat: -36.599998474121094, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.50_lon_150.10.nc\n",
      "Processed point at lat: -36.5, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.40_lon_150.10.nc\n",
      "Processed point at lat: -36.400001525878906, lon: 150.1\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_150.20.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_150.20.nc\n",
      "Processed point at lat: -38.0, lon: 150.20000000000002\n",
      "Processed point at lat: -37.900001525878906, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_150.20.nc\n",
      "Processed point at lat: -37.79999923706055, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.70_lon_150.20.nc\n",
      "Processed point at lat: -37.70000076293945, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.60_lon_150.20.nc\n",
      "Processed point at lat: -37.599998474121094, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.50_lon_150.20.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.40_lon_150.20.nc\n",
      "Processed point at lat: -37.5, lon: 150.20000000000002\n",
      "Processed point at lat: -37.400001525878906, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.30_lon_150.20.nc\n",
      "Processed point at lat: -37.29999923706055, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.10_lon_150.20.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.20_lon_150.20.nc\n",
      "Processed point at lat: -37.20000076293945, lon: 150.20000000000002\n",
      "Processed point at lat: -37.099998474121094, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.00_lon_150.20.nc\n",
      "Processed point at lat: -37.0, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.90_lon_150.20.nc\n",
      "Processed point at lat: -36.900001525878906, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.80_lon_150.20.nc\n",
      "Processed point at lat: -36.79999923706055, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.70_lon_150.20.nc\n",
      "Processed point at lat: -36.70000076293945, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.60_lon_150.20.nc\n",
      "Processed point at lat: -36.599998474121094, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.50_lon_150.20.nc\n",
      "Processed point at lat: -36.5, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.30_lon_150.20.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.40_lon_150.20.nc\n",
      "Processed point at lat: -36.400001525878906, lon: 150.20000000000002\n",
      "Processed point at lat: -36.29999923706055, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.20_lon_150.20.nc\n",
      "Processed point at lat: -36.20000076293945, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.10_lon_150.20.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-36.00_lon_150.20.nc\n",
      "Processed point at lat: -36.099998474121094, lon: 150.20000000000002\n",
      "Processed point at lat: -36.0, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-35.90_lon_150.20.nc\n",
      "Processed point at lat: -35.900001525878906, lon: 150.20000000000002\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-38.00_lon_150.30.nc\n",
      "Processed point at lat: -38.0, lon: 150.3\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.90_lon_150.30.nc\n",
      "Processed point at lat: -37.900001525878906, lon: 150.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.80_lon_150.30.nc\n",
      "Processed point at lat: -37.79999923706055, lon: 150.3\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.70_lon_150.30.nc\n",
      "Processed point at lat: -37.70000076293945, lon: 150.3\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.60_lon_150.30.nc\n",
      "Processed point at lat: -37.599998474121094, lon: 150.3\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.50_lon_150.30.nc\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.40_lon_150.30.nc\n",
      "Processed point at lat: -37.5, lon: 150.3\n",
      "Processed point at lat: -37.400001525878906, lon: 150.3\n",
      "Saved /nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT/filtered_sla_lat_-37.30_lon_150.30.nc\n",
      "Processed point at lat: -37.29999923706055, lon: 150.3\n",
      "Processing completed in 65.53 seconds.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "from ctw_functions import butter_bandpass, butter_bandpass_filter, interpolate_nan, find_nearest_non_nan\n",
    "\n",
    "# Define the vertices of the parallelogram for EAST_AUSTRALIA\n",
    "parallelogram_vertices = np.array([\n",
    "    [149, -38],  # Bottom-left\n",
    "    [158, -38],  # Bottom-right\n",
    "    [158, -25],  # Top-right\n",
    "    [149, -25]   # Top-left\n",
    "])\n",
    "\n",
    "# Check if a point is inside the parallelogram\n",
    "def is_inside_parallelogram(lat_grid, lon_grid, vertices):\n",
    "    # Mask for points inside the bounding box (min/max lat/lon)\n",
    "    mask_lon = (lon_grid >= vertices[:, 0].min()) & (lon_grid <= vertices[:, 0].max())\n",
    "    mask_lat = (lat_grid >= vertices[:, 1].min()) & (lat_grid <= vertices[:, 1].max())\n",
    "    return mask_lon & mask_lat\n",
    "\n",
    "# Function to process each valid point and save to NetCDF in parallel\n",
    "def process_grid_point(latitude, longitude, date_file_list, lowcut, highcut, fs, output_dir):\n",
    "    sla_data = []\n",
    "    date_list = []\n",
    "\n",
    "    for file_date, file_path in date_file_list:\n",
    "        dataset = xr.open_dataset(file_path)\n",
    "        \n",
    "        # Find nearest indices to the specified latitude and longitude\n",
    "        lat_idx = abs(dataset.latitude - latitude).argmin()\n",
    "        lon_idx = abs(dataset.longitude - longitude).argmin()\n",
    "\n",
    "        # Extract SLA at the specified point\n",
    "        sla_data_point = dataset['sla'].isel(latitude=lat_idx, longitude=lon_idx).values.item()\n",
    "\n",
    "        # Only proceed if the SLA point is non-NaN\n",
    "        if not np.isnan(sla_data_point):\n",
    "            sla_data.append(sla_data_point)\n",
    "            date_list.append(file_date)\n",
    "        else:\n",
    "            # Attempt to find a nearby non-NaN value\n",
    "            sla_data_point = find_nearest_non_nan(dataset, lat_idx, lon_idx)\n",
    "            if not np.isnan(sla_data_point):\n",
    "                sla_data.append(sla_data_point)\n",
    "                date_list.append(file_date)\n",
    "\n",
    "        dataset.close()\n",
    "\n",
    "    if not sla_data:  # If no valid data collected, return NaN-filled arrays\n",
    "        return latitude, longitude, np.nan, np.nan\n",
    "\n",
    "    # Convert the list to xarray DataArray with time dimension\n",
    "    time_index = pd.to_datetime(date_list)\n",
    "    sla_time_series_da = xr.DataArray(sla_data, dims=['time'], coords={'time': time_index})\n",
    "\n",
    "    # Apply the Butterworth bandpass filter if valid\n",
    "    series = sla_time_series_da.values\n",
    "    nan_count = np.isnan(series).sum()\n",
    "\n",
    "    if nan_count <= 0.9 * len(sla_time_series_da.time):\n",
    "        if nan_count > 0:\n",
    "            series = interpolate_nan(series)  # Interpolate NaNs\n",
    "        filtered_series = butter_bandpass_filter(series, lowcut, highcut, fs, order=5)\n",
    "    else:\n",
    "        filtered_series = np.full_like(series, np.nan)  # Create NaN-filled array\n",
    "\n",
    "    # Save results to NetCDF\n",
    "    save_to_netcdf(latitude, longitude, sla_time_series_da, filtered_series, output_dir)\n",
    "\n",
    "    return latitude, longitude, series, filtered_series\n",
    "\n",
    "def save_to_netcdf(lat, lon, unfiltered_series, filtered_series, output_dir):\n",
    "    \"\"\"Save the unfiltered and filtered series to NetCDF format.\"\"\"\n",
    "    file_name = f\"filtered_sla_lat_{lat:.2f}_lon_{lon:.2f}.nc\"\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Create a dataset with unfiltered and filtered SLA data\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"unfiltered_sla\": xr.DataArray(unfiltered_series.values, dims=[\"time\"], coords={\"time\": unfiltered_series.time}),\n",
    "            \"filtered_sla\": xr.DataArray(filtered_series, dims=[\"time\"], coords={\"time\": unfiltered_series.time})\n",
    "        },\n",
    "        coords={\"latitude\": lat, \"longitude\": lon}\n",
    "    )\n",
    "\n",
    "    # Save to NetCDF format\n",
    "    ds.to_netcdf(output_path)\n",
    "    #print(f\"Saved {output_path}\")\n",
    "\n",
    "# Main processing function\n",
    "def main():\n",
    "    # Load the dataset for 2023\n",
    "    base_dir = '/DGFI8/D/SWOT_L4/SWOT_Daily_Product_L4'\n",
    "    output_dir = '/nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT'\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    sample_file_path = os.path.join(base_dir, os.listdir(base_dir)[0])  # Get the first file\n",
    "    sample_dataset = xr.open_dataset(sample_file_path)\n",
    "\n",
    "    latitudes = sample_dataset.latitude.values\n",
    "    longitudes = sample_dataset.longitude.values\n",
    "\n",
    "    # Create 2D grids of lat/lon using meshgrid\n",
    "    lat_grid, lon_grid = np.meshgrid(latitudes, longitudes)\n",
    "\n",
    "    # Get the mask for valid grid points within the parallelogram\n",
    "    mask = is_inside_parallelogram(lat_grid, lon_grid, parallelogram_vertices)\n",
    "    valid_lats = lat_grid[mask]\n",
    "    valid_lons = lon_grid[mask]\n",
    "\n",
    "    # Collect all NetCDF file paths and dates\n",
    "    date_file_list = []\n",
    "    start_date = datetime(2023, 8, 29)\n",
    "    end_date = datetime(2023, 11, 30)\n",
    "    for file_name in os.listdir(base_dir):\n",
    "        if file_name.endswith('.nc'):\n",
    "            file_date_str = file_name.split('_')[-2]\n",
    "            file_date = datetime.strptime(file_date_str, \"%Y%m%d\")\n",
    "            if start_date <= file_date <= end_date:\n",
    "                file_path = os.path.join(base_dir, file_name)\n",
    "                date_file_list.append((file_date, file_path))\n",
    "\n",
    "    date_file_list.sort()\n",
    "\n",
    "    # Limit the number of points to process (test with a successful output of 100)\n",
    "    max_successful_files = 100\n",
    "    successful_files_count = 0\n",
    "\n",
    "    # Use a ProcessPoolExecutor to parallelize processing for each valid grid point\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for latitude, longitude in zip(valid_lats, valid_lons):\n",
    "            # Only submit tasks for valid grid points that are non-NaN in the original grid\n",
    "            if successful_files_count >= max_successful_files:\n",
    "                break\n",
    "\n",
    "            # Check if the SLA data point is non-NaN in the first sample dataset\n",
    "            lat_idx = abs(sample_dataset.latitude - latitude).argmin()\n",
    "            lon_idx = abs(sample_dataset.longitude - longitude).argmin()\n",
    "            sla_data_point = sample_dataset['sla'].isel(latitude=lat_idx, longitude=lon_idx).values.item()\n",
    "            \n",
    "            if not np.isnan(sla_data_point):\n",
    "                futures.append(executor.submit(process_grid_point, latitude, longitude, date_file_list, lowcut, highcut, fs, output_dir))\n",
    "                successful_files_count += 1\n",
    "\n",
    "        # Retrieve results and print summary for the first few points\n",
    "        for future in futures:\n",
    "            latitude, longitude, unfiltered, filtered = future.result()\n",
    "            #print(f\"Processed point at lat: {latitude}, lon: {longitude}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define the frequency cutoffs for the filter\n",
    "    lowcut = 0.035  # Lower cutoff in cycles per day\n",
    "    highcut = 0.15  # Upper cutoff in cycles per day\n",
    "    fs = 1.0  # Sampling frequency in cycles per day\n",
    "\n",
    "    # Start a timer to measure performance\n",
    "    start_time = time.time()\n",
    "    main()  # Call the main processing function\n",
    "    end_time = time.time()\n",
    "    #print(f\"Processing completed in {end_time - start_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8798e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimensions ('latitude',) must have the same length as the number of data dimensions, ndim=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/passaro/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/numpy/core/fromnumeric.py\", line 58, in _wrapfunc\n    return bound(*args, **kwds)\nTypeError: argmin() got an unexpected keyword argument 'out'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/passaro/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/concurrent/futures/process.py\", line 175, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"<ipython-input-3-2b14c42af9f0>\", line 25, in process_grid_point_optimized\n    lat_idx = np.argmin(np.abs(dataset.latitude - latitude))\n  File \"<__array_function__ internals>\", line 6, in argmin\n  File \"/home/passaro/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/numpy/core/fromnumeric.py\", line 1269, in argmin\n    return _wrapfunc(a, 'argmin', axis=axis, out=out)\n  File \"/home/passaro/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/numpy/core/fromnumeric.py\", line 67, in _wrapfunc\n    return _wrapit(obj, method, *args, **kwds)\n  File \"/home/passaro/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/numpy/core/fromnumeric.py\", line 48, in _wrapit\n    result = wrap(result)\n  File \"/home/passaro/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/xarray/core/dataarray.py\", line 2979, in __array_wrap__\n    new_var = self.variable.__array_wrap__(obj, context)\n  File \"/home/passaro/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/xarray/core/variable.py\", line 2312, in __array_wrap__\n    return Variable(self.dims, obj)\n  File \"/home/passaro/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/xarray/core/variable.py\", line 315, in __init__\n    self._dims = self._parse_dimensions(dims)\n  File \"/home/passaro/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/xarray/core/variable.py\", line 575, in _parse_dimensions\n    f\"dimensions {dims} must have the same length as the \"\nValueError: dimensions ('latitude',) must have the same length as the number of data dimensions, ndim=0\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2b14c42af9f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mmain_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing completed in {end_time - start_time:.2f} seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2b14c42af9f0>\u001b[0m in \u001b[0;36mmain_optimized\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mlatitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munfiltered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;31m# Only count if the processing was successful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_process_worker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcall_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcall_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ExceptionWithTraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2b14c42af9f0>\u001b[0m in \u001b[0;36mprocess_grid_point_optimized\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'latitude'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'longitude'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mlat_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatitude\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlatitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mlon_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlongitude\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlongitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmin\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmin\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     \"\"\"\n\u001b[0;32m-> 1269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Call _wrapit from within the except clause to ensure a potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# exception has a traceback chain.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m__array_wrap__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataArray\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m         \u001b[0mnew_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m__array_wrap__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_unary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    313\u001b[0m         \"\"\"\n\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_compatible_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning_altimetry_validation/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m_parse_dimensions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             raise ValueError(\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;34mf\"dimensions {dims} must have the same length as the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 \u001b[0;34mf\"number of data dimensions, ndim={self.ndim}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: dimensions ('latitude',) must have the same length as the number of data dimensions, ndim=0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from scipy.signal import butter, filtfilt\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from ctw_functions import butter_bandpass, butter_bandpass_filter, interpolate_nan, find_nearest_non_nan\n",
    "\n",
    "# Improved function to process grid points\n",
    "def process_grid_point_optimized(latitude, longitude, date_file_list, lowcut, highcut, fs, output_dir):\n",
    "    \"\"\"\n",
    "    Optimized grid point processing that caches files and reduces I/O.\n",
    "    \"\"\"\n",
    "    sla_data = []\n",
    "    date_list = []\n",
    "\n",
    "    for file_date, file_path in date_file_list:\n",
    "        # Use dask-backed loading (lazy load) with chunks\n",
    "        dataset = xr.open_dataset(file_path, chunks={'latitude': 50, 'longitude': 50})\n",
    "        \n",
    "        lat_idx = np.argmin(np.abs(dataset.latitude - latitude))\n",
    "        lon_idx = np.argmin(np.abs(dataset.longitude - longitude))\n",
    "\n",
    "        sla_data_point = dataset['sla'].isel(latitude=lat_idx, longitude=lon_idx).values.item()\n",
    "\n",
    "        if not np.isnan(sla_data_point):\n",
    "            sla_data.append(sla_data_point)\n",
    "            date_list.append(file_date)\n",
    "        else:\n",
    "            sla_data_point = find_nearest_non_nan(dataset, lat_idx, lon_idx)\n",
    "            if not np.isnan(sla_data_point):\n",
    "                sla_data.append(sla_data_point)\n",
    "                date_list.append(file_date)\n",
    "                \n",
    "        dataset.close()\n",
    "\n",
    "    if not sla_data:\n",
    "        return latitude, longitude, np.nan, np.nan\n",
    "\n",
    "    time_index = pd.to_datetime(date_list)\n",
    "    sla_time_series_da = xr.DataArray(sla_data, dims=['time'], coords={'time': time_index})\n",
    "\n",
    "    series = sla_time_series_da.values\n",
    "    nan_count = np.isnan(series).sum()\n",
    "\n",
    "    if nan_count <= 0.9 * len(sla_time_series_da.time):\n",
    "        if nan_count > 0:\n",
    "            series = interpolate_nan(series)\n",
    "        filtered_series = butter_bandpass_filter(series, lowcut, highcut, fs, order=5)\n",
    "    else:\n",
    "        filtered_series = np.full_like(series, np.nan)\n",
    "\n",
    "    save_to_netcdf(latitude, longitude, sla_time_series_da, filtered_series, output_dir)\n",
    "\n",
    "    return latitude, longitude, series, filtered_series\n",
    "\n",
    "def save_to_netcdf(lat, lon, unfiltered_series, filtered_series, output_dir):\n",
    "    \"\"\"Save the unfiltered and filtered series to NetCDF format.\"\"\"\n",
    "    file_name = f\"filtered_sla_lat_{lat:.2f}_lon_{lon:.2f}.nc\"\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Create a dataset with unfiltered and filtered SLA data\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"unfiltered_sla\": xr.DataArray(unfiltered_series.values, dims=[\"time\"], coords={\"time\": unfiltered_series.time}),\n",
    "            \"filtered_sla\": xr.DataArray(filtered_series, dims=[\"time\"], coords={\"time\": unfiltered_series.time})\n",
    "        },\n",
    "        coords={\"latitude\": lat, \"longitude\": lon}\n",
    "    )\n",
    "\n",
    "    # Save to NetCDF format\n",
    "    ds.to_netcdf(output_path)\n",
    "    print(f\"Saved {output_path}\")\n",
    "\n",
    "# Main function with dask integration and limit of 100 successful files\n",
    "def main_optimized():\n",
    "    base_dir = '/DGFI8/D/SWOT_L4/SWOT_Daily_Product_L4'\n",
    "    output_dir = '/nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    sample_file_path = os.path.join(base_dir, os.listdir(base_dir)[0])\n",
    "    sample_dataset = xr.open_dataset(sample_file_path, chunks={'latitude': 50, 'longitude': 50})\n",
    "\n",
    "    latitudes = sample_dataset.latitude.values\n",
    "    longitudes = sample_dataset.longitude.values\n",
    "    lat_grid, lon_grid = np.meshgrid(latitudes, longitudes)\n",
    "\n",
    "    mask = is_inside_parallelogram(lat_grid, lon_grid, parallelogram_vertices)\n",
    "    valid_lats = lat_grid[mask]\n",
    "    valid_lons = lon_grid[mask]\n",
    "\n",
    "    date_file_list = []\n",
    "    start_date = datetime(2023, 8, 29)\n",
    "    end_date = datetime(2023, 11, 30)\n",
    "    for file_name in os.listdir(base_dir):\n",
    "        if file_name.endswith('.nc'):\n",
    "            file_date_str = file_name.split('_')[-2]\n",
    "            file_date = datetime.strptime(file_date_str, \"%Y%m%d\")\n",
    "            if start_date <= file_date <= end_date:\n",
    "                file_path = os.path.join(base_dir, file_name)\n",
    "                date_file_list.append((file_date, file_path))\n",
    "\n",
    "    date_file_list.sort()\n",
    "    \n",
    "    successful_files_count = 0\n",
    "    max_successful_files = 100  # Limit to 100 successful files\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for latitude, longitude in zip(valid_lats, valid_lons):\n",
    "            if successful_files_count >= max_successful_files:\n",
    "                break\n",
    "\n",
    "            futures.append(executor.submit(process_grid_point_optimized, latitude, longitude, date_file_list, lowcut, highcut, fs, output_dir))\n",
    "\n",
    "        # Track progress using dask progress bar\n",
    "        with ProgressBar():\n",
    "            for future in futures:\n",
    "                latitude, longitude, unfiltered, filtered = future.result()\n",
    "\n",
    "                # Only count if the processing was successful\n",
    "                if not np.isnan(unfiltered).all():\n",
    "                    successful_files_count += 1\n",
    "                    print(f\"Processed point at lat: {latitude}, lon: {longitude}\")\n",
    "                \n",
    "                # Stop if the limit is reached\n",
    "                if successful_files_count >= max_successful_files:\n",
    "                    print(\"Reached limit of 100 successful files.\")\n",
    "                    break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lowcut = 0.035\n",
    "    highcut = 0.15\n",
    "    fs = 1.0\n",
    "\n",
    "    start_time = time.time()\n",
    "    main_optimized()\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e9d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(ctw_functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230b3df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Function to check if a point is inside the parallelogram\n",
    "def is_inside_parallelogram(lat, lon, vertices):\n",
    "    \"\"\"\n",
    "    Check if a point (lat, lon) is inside the defined parallelogram\n",
    "    \"\"\"\n",
    "    if lon < np.min(vertices[:, 0]) or lon > np.max(vertices[:, 0]):\n",
    "        return False\n",
    "    if lat < np.min(vertices[:, 1]) or lat > np.max(vertices[:, 1]):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Function to reconstruct daily grids from filtered time series\n",
    "def reconstruct_daily_grids(input_dir, original_grid_file, output_dir, start_date, end_date, parallelogram_vertices, test_days=5):\n",
    "    \"\"\"\n",
    "    Rebuild daily grids from saved time series of each grid point.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dir: Directory where the time series for each grid point is stored.\n",
    "    - original_grid_file: Path to a sample original NetCDF file for structure.\n",
    "    - output_dir: Directory where the reconstructed daily grids will be saved.\n",
    "    - start_date: Start date for reconstruction (datetime).\n",
    "    - end_date: End date for reconstruction (datetime).\n",
    "    - parallelogram_vertices: Array defining the vertices of the parallelogram for spatial filtering.\n",
    "    - test_days: Number of days to process for testing purposes.\n",
    "    \"\"\"\n",
    "    # Start a timer to measure performance\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load the original grid to get the spatial structure (lat/lon dimensions)\n",
    "    original_dataset = xr.open_dataset(original_grid_file)\n",
    "    latitudes = original_dataset.latitude.values\n",
    "    longitudes = original_dataset.longitude.values\n",
    "\n",
    "    # Filter latitudes and longitudes based on the parallelogram\n",
    "    lat_indices, lon_indices = [], []\n",
    "    for lat_idx, lat in enumerate(latitudes):\n",
    "        for lon_idx, lon in enumerate(longitudes):\n",
    "            if is_inside_parallelogram(lat, lon, parallelogram_vertices):\n",
    "                lat_indices.append(lat_idx)\n",
    "                lon_indices.append(lon_idx)\n",
    "\n",
    "    # Reduce latitudes and longitudes to the region of interest\n",
    "    valid_latitudes = latitudes[np.unique(lat_indices)]\n",
    "    valid_longitudes = longitudes[np.unique(lon_indices)]\n",
    "\n",
    "    # Create a time range for the target dates\n",
    "    time_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "    # Limit the time range to the specified number of test days\n",
    "    time_range = time_range[:test_days]\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize a dictionary to store all the time series data\n",
    "    time_series_data = {}\n",
    "\n",
    "    # Load all time series files (assuming format filtered_sla_lat_xx_lon_xx.nc) from input directory\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.startswith('filtered_sla') and file_name.endswith('.nc'):\n",
    "            lat_lon_str = file_name.replace('filtered_sla_lat_', '').replace('.nc', '')\n",
    "            lat_str, lon_str = lat_lon_str.split('_lon_')\n",
    "            lat, lon = float(lat_str), float(lon_str)\n",
    "\n",
    "            # Load the time series for this grid point\n",
    "            ds = xr.open_dataset(os.path.join(input_dir, file_name))\n",
    "            time_series_data[(lat, lon)] = ds['filtered_sla'].values\n",
    "            ds.close()\n",
    "\n",
    "    # For each day in the range, reconstruct the grid and save as NetCDF\n",
    "    for day in time_range:\n",
    "        print(f\"Reconstructing grid for {day}\")\n",
    "\n",
    "        # Initialize an empty array for the grid (valid lat, valid lon)\n",
    "        grid = np.full((len(valid_latitudes), len(valid_longitudes)), np.nan)\n",
    "\n",
    "        # Iterate over each valid grid point and assign the value for the current day\n",
    "        for lat_idx, lat in enumerate(valid_latitudes):\n",
    "            for lon_idx, lon in enumerate(valid_longitudes):\n",
    "                if (lat, lon) in time_series_data:\n",
    "                    # Find the corresponding value for the current day\n",
    "                    try:\n",
    "                        day_index = np.where(ds.time == np.datetime64(day))[0][0]\n",
    "                        grid[lat_idx, lon_idx] = time_series_data[(lat, lon)][day_index]\n",
    "                    except IndexError:\n",
    "                        # In case the day doesn't exist in the time series, leave as NaN\n",
    "                        pass\n",
    "\n",
    "        # Create a new dataset for the current day\n",
    "        filtered_dataset = xr.Dataset(\n",
    "            {\n",
    "                'sla': (['latitude', 'longitude'], grid)\n",
    "            },\n",
    "            coords={\n",
    "                'latitude': valid_latitudes,\n",
    "                'longitude': valid_longitudes,\n",
    "                'time': pd.to_datetime([day])\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Save the reconstructed grid for this day\n",
    "        output_file = os.path.join(output_dir, f\"filtered_grid_{day.strftime('%Y%m%d')}.nc\")\n",
    "        filtered_dataset.to_netcdf(output_file)\n",
    "        print(f\"Saved reconstructed grid for {day} to {output_file}\")\n",
    "\n",
    "    original_dataset.close()\n",
    "\n",
    "    # End the timer and print the total processing time\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total processing time for reconstruction: {total_time:.2f} seconds\")\n",
    "\n",
    "# Main function to drive the reconstruction process\n",
    "def main():\n",
    "    input_dir = '/nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT'\n",
    "    output_dir = '/nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_daily_grids_SWOT_reconstructed'\n",
    "    \n",
    "    # Dynamically select any original grid file from the specified directory\n",
    "    original_grid_dir = '/DGFI8/D/SWOT_L4/SWOT_Daily_Product_L4'\n",
    "    original_grid_files = [f for f in os.listdir(original_grid_dir) if f.endswith('.nc')]\n",
    "    \n",
    "    if not original_grid_files:\n",
    "        raise FileNotFoundError(\"No original grid files found in the specified directory.\")\n",
    "\n",
    "    original_grid_file = os.path.join(original_grid_dir, original_grid_files[0])  # Use the first file found\n",
    "    print(f\"Using original grid file: {original_grid_file}\")\n",
    "\n",
    "    start_date = datetime(2023, 8, 29)\n",
    "    end_date = datetime(2023, 11, 30)\n",
    "\n",
    "    # Define the vertices of the parallelogram (EAST_AUSTRALIA region)\n",
    "    parallelogram_vertices = np.array([\n",
    "        [149, -38],  # Bottom-left\n",
    "        [158, -38],  # Bottom-right\n",
    "        [158, -25],  # Top-right\n",
    "        [149, -25]   # Top-left\n",
    "    ])\n",
    "\n",
    "    # Run reconstruction with a limit on the number of days to process\n",
    "    reconstruct_daily_grids(input_dir, original_grid_file, output_dir, start_date, end_date, parallelogram_vertices, test_days=5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Function to plot a filtered time series for a specific grid point\n",
    "def plot_filtered_time_series(input_dir, latitude, longitude):\n",
    "    \"\"\"\n",
    "    Plot the filtered time series for a specific latitude and longitude.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dir: Directory containing the filtered time series files.\n",
    "    - latitude: Latitude of the grid point to plot.\n",
    "    - longitude: Longitude of the grid point to plot.\n",
    "    \"\"\"\n",
    "    # Construct the filename based on the latitude and longitude\n",
    "    lat_str = f\"{latitude:.2f}\"\n",
    "    lon_str = f\"{longitude:.2f}\"\n",
    "    file_name = f\"filtered_sla_lat_{lat_str}_lon_{lon_str}.nc\"\n",
    "    file_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    # Load the filtered time series dataset\n",
    "    dataset = xr.open_dataset(file_path)\n",
    "\n",
    "    # Extract the time series data\n",
    "    time_series = dataset['filtered_sla'].values  # Ensure this matches your variable name\n",
    "    time = dataset['time'].values\n",
    "\n",
    "    # Close the dataset after loading the data\n",
    "    dataset.close()\n",
    "\n",
    "    # Plot the time series\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time, time_series, marker='o', linestyle='-')\n",
    "    plt.title(f'Filtered Sea Level Anomaly Time Series at (Lat: {latitude}, Lon: {longitude})')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Filtered Sea Level Anomaly (m)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to randomly select a file and plot the corresponding time series\n",
    "def plot_random_filtered_time_series(input_dir):\n",
    "    \"\"\"\n",
    "    Randomly select a filtered time series file and plot its data.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dir: Directory containing the filtered time series files.\n",
    "    \"\"\"\n",
    "    # List all NetCDF files in the input directory\n",
    "    filtered_files = [f for f in os.listdir(input_dir) if f.endswith('.nc')]\n",
    "\n",
    "    # Check if there are any filtered files\n",
    "    if not filtered_files:\n",
    "        print(\"No filtered files found in the directory.\")\n",
    "        return\n",
    "\n",
    "    # Randomly select a file\n",
    "    selected_file = random.choice(filtered_files)\n",
    "    print(f\"Selected file: {selected_file}\")\n",
    "\n",
    "    # Use regex to extract latitude and longitude from the filename\n",
    "    match = re.search(r'lat_([-+]?\\d*\\.\\d+|[-+]?\\d+)_lon_([-+]?\\d*\\.\\d+|[-+]?\\d+)', selected_file)\n",
    "    if match:\n",
    "        latitude = float(match.group(1))\n",
    "        longitude = float(match.group(2))\n",
    "    else:\n",
    "        print(\"Could not extract latitude and longitude from the filename.\")\n",
    "        return\n",
    "\n",
    "    # Call the plotting function with the selected latitude and longitude\n",
    "    plot_filtered_time_series(input_dir, latitude, longitude)\n",
    "\n",
    "# Example usage\n",
    "input_dir = '/nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_grids_SWOT'\n",
    "\n",
    "# Call the function to plot a random filtered time series\n",
    "plot_random_filtered_time_series(input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "\n",
    "def plot_reconstructed_grid(nc_file):\n",
    "    # Load the dataset\n",
    "    dataset = xr.open_dataset(nc_file)\n",
    "\n",
    "    # Extract the data for plotting\n",
    "    grid_data = dataset['sla'].values\n",
    "    latitudes = dataset['latitude'].values\n",
    "    longitudes = dataset['longitude'].values\n",
    "    time = dataset['time'].values\n",
    "\n",
    "    # Create a map with Cartopy\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.set_extent([longitudes.min(), longitudes.max(), latitudes.min(), latitudes.max()], crs=ccrs.PlateCarree())\n",
    "\n",
    "    # Add coastlines and gridlines\n",
    "    ax.coastlines()\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    # Plot the data\n",
    "    im = ax.pcolormesh(longitudes, latitudes, grid_data, shading='auto', cmap='viridis', transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # Add a colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, orientation='vertical', pad=0.02, aspect=50)\n",
    "    cbar.set_label('Sea Level Anomaly (SLA) [m]')\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(f'Sea Level Anomaly - {np.datetime_as_string(time[0], unit=\"D\")}', fontsize=16)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# File path to the reconstructed grid\n",
    "nc_file = '/nfs/DGFI8/H/work_marcello/coastal_trapped_waves_data/filtered_daily_grids_SWOT_reconstructed/filtered_grid_20230829.nc'\n",
    "\n",
    "# Plot the reconstructed grid\n",
    "plot_reconstructed_grid(nc_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b71d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning_altimetry_validation] *",
   "language": "python",
   "name": "conda-env-machine_learning_altimetry_validation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
